{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a14918-a79a-4b6a-8c87-c112504ba72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ================================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ETL PIPELINE EXECUTION STARTED\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ================================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Connected to database: supply_chain_db\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - \n",
      "======================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Processing table: suppliers\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Extracting data from table: suppliers\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Extracted 30 rows from suppliers\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ No duplicates found\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Standardized date column: created_date\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Adding derived columns for suppliers\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Applying business rules for suppliers\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - === Validating data quality for: suppliers ===\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Data quality validation PASSED for suppliers\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Validation passed for suppliers\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - \n",
      "======================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Processing table: products\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Extracting data from table: products\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Extracted 46 rows from products\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ No duplicates found\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Adding derived columns for products\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Applying business rules for products\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - === Validating data quality for: products ===\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Data quality validation PASSED for products\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Validation passed for products\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - \n",
      "======================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Processing table: inventory\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Extracting data from table: inventory\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Extracted 263 rows from inventory\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ No duplicates found\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Standardized date column: snapshot_date\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Adding derived columns for inventory\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Added 'quantity_available' column\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Applying business rules for inventory\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - === Validating data quality for: inventory ===\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Data quality validation PASSED for inventory\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Validation passed for inventory\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - \n",
      "======================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Processing table: orders\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Extracting data from table: orders\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Extracted 67 rows from orders\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ No duplicates found\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Standardized date column: order_date\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Standardized date column: expected_delivery_date\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Standardized date column: actual_delivery_date\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Adding derived columns for orders\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Added delivery metrics columns\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Applying business rules for orders\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - === Validating data quality for: orders ===\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - WARNING - ⚠ High null count in 'actual_delivery_date': 31 (46.3%)\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - WARNING - ⚠ High null count in 'delivery_delay_days': 31 (46.3%)\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - ERROR - ✗ Data quality validation FAILED for orders\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - WARNING - ⚠ Validation issues found in orders\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - WARNING -   - Column 'actual_delivery_date' has 46.3% nulls (threshold: 5.0%)\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - WARNING -   - Column 'delivery_delay_days' has 46.3% nulls (threshold: 5.0%)\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - \n",
      "======================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Processing table: sales\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Extracting data from table: sales\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Extracted 166 rows from sales\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ No duplicates found\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Standardized date column: sale_date\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Adding derived columns for sales\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Added 'unit_price' column\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Applying business rules for sales\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - === Validating data quality for: sales ===\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Data quality validation PASSED for sales\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Validation passed for sales\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ Database connection closed\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - \n",
      "================================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ETL PIPELINE EXECUTION SUMMARY\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ================================================================================\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Start time: 2026-02-01 23:52:44.280565\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - End time: 2026-02-01 23:52:44.582971\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Duration: 0.30 seconds\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Tables processed: 5\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Total rows extracted: 572\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - Total rows loaded: 0\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ✓ All tables processed successfully\n",
      "2026-02-01 23:52:44 - ETL_Pipeline - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables processed: 5\n",
      "Total rows extracted: 572\n",
      "Duration: 0.30s\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "from pipeline import ETLPipeline, setup_logging\n",
    "from etl import ETLConfig\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging(log_level='INFO', log_file='etl_execution.log')\n",
    "\n",
    "# Create configuration\n",
    "config = ETLConfig(\n",
    "    db_host='mysql',\n",
    "    db_port=3306,\n",
    "    db_name='supply_chain_db'\n",
    ")\n",
    "\n",
    "# Cell 2: Run pipeline\n",
    "pipeline = ETLPipeline(config, logger)\n",
    "\n",
    "tables_to_process = ['suppliers', 'products', 'inventory', 'orders', 'sales']\n",
    "\n",
    "stats = pipeline.run_full_pipeline(\n",
    "    tables=tables_to_process,\n",
    "    enable_validation=True,\n",
    "    enable_transformation=True\n",
    ")\n",
    "\n",
    "# Cell 3: View results\n",
    "print(f\"Tables processed: {stats['tables_processed']}\")\n",
    "print(f\"Total rows extracted: {stats['total_rows_extracted']:,}\")\n",
    "print(f\"Duration: {(stats['end_time'] - stats['start_time']).total_seconds():.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e34cc8bb-01bf-4971-b1ab-e232a572f930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 23:55:45 - ETL_Pipeline - INFO - ✓ Connected to database: supply_chain_db\n",
      "2026-02-01 23:55:45 - ETL_Pipeline - INFO - ✓ Connected to database: supply_chain_db\n",
      "2026-02-01 23:55:45 - ETL_Pipeline - INFO - Extracting data from table: products\n",
      "2026-02-01 23:55:45 - ETL_Pipeline - INFO - Extracting data from table: products\n",
      "2026-02-01 23:55:45 - ETL_Pipeline - INFO - ✓ Extracted 46 rows from products\n",
      "2026-02-01 23:55:45 - ETL_Pipeline - INFO - ✓ Extracted 46 rows from products\n",
      "2026-02-01 23:55:45 - ETL_Pipeline - INFO - Extracting data from table: sales\n",
      "2026-02-01 23:55:45 - ETL_Pipeline - INFO - Extracting data from table: sales\n",
      "2026-02-01 23:55:45 - ETL_Pipeline - INFO - ✓ Extracted 166 rows from sales\n",
      "2026-02-01 23:55:45 - ETL_Pipeline - INFO - ✓ Extracted 166 rows from sales\n",
      "2026-02-01 23:55:45 - ETL_Pipeline - INFO - ✓ Database connection closed\n",
      "2026-02-01 23:55:45 - ETL_Pipeline - INFO - ✓ Database connection closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products: 46 rows, 6 columns\n",
      "Sales: 166 rows, 6 columns\n",
      "✓ HTML report generated: supply_chain_quality_report.html\n",
      "✓ Report exported to: quality_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "from etl import ETLConfig, DatabaseConnection, DataExtractor\n",
    "from quality.rules_engine import DataQualityRulesEngine\n",
    "from quality.profiler import DataProfiler\n",
    "from quality.anomaly import AnomalyDetector\n",
    "from quality.reporter import DataQualityReporter\n",
    "from pipeline import setup_logging\n",
    "\n",
    "logger = setup_logging()\n",
    "config = ETLConfig()\n",
    "\n",
    "# Cell 2: Extract data\n",
    "with DatabaseConnection(config, logger) as db:\n",
    "    extractor = DataExtractor(db, logger)\n",
    "    products_df = extractor.extract_table('products')\n",
    "    sales_df = extractor.extract_table('sales')\n",
    "\n",
    "# Cell 3: Run validation rules\n",
    "rules_engine = DataQualityRulesEngine()\n",
    "rules_engine.define_standard_rules()\n",
    "\n",
    "products_results = rules_engine.execute_rules(products_df, 'products')\n",
    "sales_results = rules_engine.execute_rules(sales_df, 'sales')\n",
    "\n",
    "# View summary\n",
    "summary_df = rules_engine.get_summary()\n",
    "summary_df\n",
    "\n",
    "# Cell 4: Profile data\n",
    "products_profile = DataProfiler.profile_dataframe(products_df, 'products')\n",
    "sales_profile = DataProfiler.profile_dataframe(sales_df, 'sales')\n",
    "\n",
    "print(f\"Products: {products_profile['row_count']} rows, {products_profile['column_count']} columns\")\n",
    "print(f\"Sales: {sales_profile['row_count']} rows, {sales_profile['column_count']} columns\")\n",
    "\n",
    "# Cell 5: Detect anomalies\n",
    "products_anomalies = AnomalyDetector.analyze_table_anomalies(products_df, 'products')\n",
    "sales_anomalies = AnomalyDetector.analyze_table_anomalies(sales_df, 'sales')\n",
    "\n",
    "anomaly_summary = AnomalyDetector.get_outlier_summary(products_anomalies)\n",
    "anomaly_summary\n",
    "\n",
    "# Cell 6: Generate comprehensive report\n",
    "all_results = products_results + sales_results\n",
    "all_profiles = {'products': products_profile, 'sales': sales_profile}\n",
    "all_anomalies = {'products': products_anomalies, 'sales': sales_anomalies}\n",
    "\n",
    "# Generate HTML report\n",
    "DataQualityReporter.generate_html_report(\n",
    "    validation_results=all_results,\n",
    "    profiles=all_profiles,\n",
    "    anomalies=all_anomalies,\n",
    "    output_path='supply_chain_quality_report.html'\n",
    ")\n",
    "\n",
    "# Export to JSON\n",
    "summary_report = DataQualityReporter.generate_summary_report(\n",
    "    validation_results=all_results,\n",
    "    profiles=all_profiles,\n",
    "    anomalies=all_anomalies\n",
    ")\n",
    "DataQualityReporter.export_to_json(summary_report, 'quality_summary.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3605201f-b9f7-4d18-9955-984abb1c340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - ✓ Connected to database: supply_chain_db\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - ✓ Connected to database: supply_chain_db\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - ✓ Connected to database: supply_chain_db\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - Extracting data from table: products\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - Extracting data from table: products\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - Extracting data from table: products\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - ✓ Extracted 46 rows from products\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - ✓ Extracted 46 rows from products\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - ✓ Extracted 46 rows from products\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - Extracting data from table: sales\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - Extracting data from table: sales\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - Extracting data from table: sales\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - ✓ Extracted 166 rows from sales\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - ✓ Extracted 166 rows from sales\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - ✓ Extracted 166 rows from sales\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - ✓ Database connection closed\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - ✓ Database connection closed\n",
      "2026-02-01 23:56:16 - ETL_Pipeline - INFO - ✓ Database connection closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PASS - reorder_level_reasonable: ✓ PASS: Reorder level should be at least 10\n",
      "✓ PASS - revenue_matches_calculation: ✓ PASS: Revenue should match quantity * unit_price\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "from etl import ETLConfig, DatabaseConnection, DataExtractor\n",
    "from quality.rules_engine import DataQualityRulesEngine, ValidationRule\n",
    "from pipeline import setup_logging\n",
    "\n",
    "logger = setup_logging()\n",
    "config = ETLConfig()\n",
    "\n",
    "# Cell 2: Define custom rules\n",
    "rules_engine = DataQualityRulesEngine()\n",
    "\n",
    "# Custom rule: Products with very low reorder levels\n",
    "rules_engine.add_rule('products', ValidationRule(\n",
    "    rule_name='reorder_level_reasonable',\n",
    "    rule_type='validity',\n",
    "    column='reorder_level',\n",
    "    condition=lambda df: df['reorder_level'] >= 10,\n",
    "    severity='WARNING',\n",
    "    description='Reorder level should be at least 10'\n",
    "))\n",
    "\n",
    "# Custom rule: Sales revenue matches calculated value\n",
    "rules_engine.add_rule('sales', ValidationRule(\n",
    "    rule_name='revenue_matches_calculation',\n",
    "    rule_type='consistency',\n",
    "    column='revenue',\n",
    "    condition=lambda df: abs(df['revenue'] - (df['quantity_sold'] * df['revenue'] / df['quantity_sold'])) < 0.01,\n",
    "    severity='CRITICAL',\n",
    "    description='Revenue should match quantity * unit_price'\n",
    "))\n",
    "\n",
    "# Cell 3: Execute custom rules\n",
    "with DatabaseConnection(config, logger) as db:\n",
    "    extractor = DataExtractor(db, logger)\n",
    "    products_df = extractor.extract_table('products')\n",
    "    sales_df = extractor.extract_table('sales')\n",
    "\n",
    "products_results = rules_engine.execute_rules(products_df, 'products')\n",
    "sales_results = rules_engine.execute_rules(sales_df, 'sales')\n",
    "\n",
    "# Cell 4: View results\n",
    "for result in products_results + sales_results:\n",
    "    status = \"✓ PASS\" if result.passed else \"✗ FAIL\"\n",
    "    print(f\"{status} - {result.rule_name}: {result.message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c22eb0-318e-45e0-a388-d3a66ab1f5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb3f3a-826b-4bd0-8215-af119ef4c2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613588e-c47d-4205-a1ae-5ace3d9fad70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce507e1-14cc-47a6-835f-416a7501f37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79281e-65c3-4b04-aa30-9af1b04660dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec6807-7924-489b-9ef5-0c5316706801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb12dff-db89-4188-81a8-b562b3405a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
