{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8c3f9a7-b20e-4d3e-9200-70ae5e72fa5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Framework - Ready for integration with ETL Pipeline\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Supply Chain Intelligence Hub - Data Quality Framework\n",
    "======================================================\n",
    "Advanced data quality validation framework with profiling,\n",
    "anomaly detection, and automated reporting.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import json\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationRule:\n",
    "    \"\"\"Individual validation rule definition\"\"\"\n",
    "    rule_name: str\n",
    "    rule_type: str\n",
    "    column: Optional[str] = None\n",
    "    threshold: Optional[float] = None\n",
    "    condition: Optional[Callable] = None\n",
    "    severity: str = 'WARNING'\n",
    "    description: str = \"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Result of a validation rule execution\"\"\"\n",
    "    rule_name: str\n",
    "    passed: bool\n",
    "    severity: str\n",
    "    message: str\n",
    "    affected_rows: int = 0\n",
    "    affected_percentage: float = 0.0\n",
    "    details: Dict = field(default_factory=dict)\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "\n",
    "class DataQualityRulesEngine:\n",
    "    \"\"\"Engine to define and execute data quality rules\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rules: Dict[str, List[ValidationRule]] = {}\n",
    "        self.results: List[ValidationResult] = []\n",
    "    \n",
    "    def add_rule(self, table_name: str, rule: ValidationRule):\n",
    "        \"\"\"Add validation rule for a specific table\"\"\"\n",
    "        if table_name not in self.rules:\n",
    "            self.rules[table_name] = []\n",
    "        self.rules[table_name].append(rule)\n",
    "    \n",
    "    def define_standard_rules(self):\n",
    "        \"\"\"Define standard data quality rules for all tables\"\"\"\n",
    "        \n",
    "        # SUPPLIERS\n",
    "        self.add_rule('suppliers', ValidationRule(\n",
    "            rule_name='supplier_id_unique',\n",
    "            rule_type='uniqueness',\n",
    "            column='supplier_id',\n",
    "            severity='CRITICAL',\n",
    "            description='Supplier ID must be unique'\n",
    "        ))\n",
    "        \n",
    "        self.add_rule('suppliers', ValidationRule(\n",
    "            rule_name='reliability_score_range',\n",
    "            rule_type='validity',\n",
    "            column='reliability_score',\n",
    "            condition=lambda df: (df['reliability_score'] >= 0) & (df['reliability_score'] <= 100),\n",
    "            severity='CRITICAL',\n",
    "            description='Reliability score must be between 0 and 100'\n",
    "        ))\n",
    "        \n",
    "        # PRODUCTS\n",
    "        self.add_rule('products', ValidationRule(\n",
    "            rule_name='product_id_unique',\n",
    "            rule_type='uniqueness',\n",
    "            column='product_id',\n",
    "            severity='CRITICAL',\n",
    "            description='Product ID must be unique'\n",
    "        ))\n",
    "        \n",
    "        self.add_rule('products', ValidationRule(\n",
    "            rule_name='unit_cost_positive',\n",
    "            rule_type='validity',\n",
    "            column='unit_cost',\n",
    "            condition=lambda df: df['unit_cost'] > 0,\n",
    "            severity='CRITICAL',\n",
    "            description='Unit cost must be positive'\n",
    "        ))\n",
    "        \n",
    "        # INVENTORY\n",
    "        self.add_rule('inventory', ValidationRule(\n",
    "            rule_name='quantity_on_hand_valid',\n",
    "            rule_type='validity',\n",
    "            column='quantity_on_hand',\n",
    "            condition=lambda df: df['quantity_on_hand'] >= 0,\n",
    "            severity='CRITICAL',\n",
    "            description='Quantity on hand cannot be negative'\n",
    "        ))\n",
    "        \n",
    "        self.add_rule('inventory', ValidationRule(\n",
    "            rule_name='reserved_not_exceed_onhand',\n",
    "            rule_type='consistency',\n",
    "            column='quantity_reserved',\n",
    "            condition=lambda df: df['quantity_reserved'] <= df['quantity_on_hand'],\n",
    "            severity='CRITICAL',\n",
    "            description='Reserved quantity cannot exceed quantity on hand'\n",
    "        ))\n",
    "        \n",
    "        # ORDERS\n",
    "        self.add_rule('orders', ValidationRule(\n",
    "            rule_name='order_quantity_positive',\n",
    "            rule_type='validity',\n",
    "            column='order_quantity',\n",
    "            condition=lambda df: df['order_quantity'] > 0,\n",
    "            severity='CRITICAL',\n",
    "            description='Order quantity must be positive'\n",
    "        ))\n",
    "        \n",
    "        # SALES\n",
    "        self.add_rule('sales', ValidationRule(\n",
    "            rule_name='quantity_sold_positive',\n",
    "            rule_type='validity',\n",
    "            column='quantity_sold',\n",
    "            condition=lambda df: df['quantity_sold'] > 0,\n",
    "            severity='CRITICAL',\n",
    "            description='Quantity sold must be positive'\n",
    "        ))\n",
    "    \n",
    "    def execute_rules(self, df: pd.DataFrame, table_name: str) -> List[ValidationResult]:\n",
    "        \"\"\"Execute all rules for a given table\"\"\"\n",
    "        if table_name not in self.rules:\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        total_rows = len(df)\n",
    "        \n",
    "        for rule in self.rules[table_name]:\n",
    "            try:\n",
    "                if rule.rule_type == 'uniqueness':\n",
    "                    result = self._check_uniqueness(df, rule, total_rows)\n",
    "                elif rule.rule_type == 'completeness':\n",
    "                    result = self._check_completeness(df, rule, total_rows)\n",
    "                elif rule.rule_type in ['validity', 'consistency']:\n",
    "                    result = self._check_condition(df, rule, total_rows)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                results.append(ValidationResult(\n",
    "                    rule_name=rule.rule_name,\n",
    "                    passed=False,\n",
    "                    severity='CRITICAL',\n",
    "                    message=f\"Rule execution failed: {str(e)}\"\n",
    "                ))\n",
    "        \n",
    "        self.results.extend(results)\n",
    "        return results\n",
    "    \n",
    "    def _check_uniqueness(self, df: pd.DataFrame, rule: ValidationRule, \n",
    "                         total_rows: int) -> ValidationResult:\n",
    "        \"\"\"Check if column values are unique\"\"\"\n",
    "        duplicates = df[rule.column].duplicated().sum()\n",
    "        passed = duplicates == 0\n",
    "        \n",
    "        return ValidationResult(\n",
    "            rule_name=rule.rule_name,\n",
    "            passed=passed,\n",
    "            severity=rule.severity,\n",
    "            message=f\"{'‚úì PASS' if passed else '‚úó FAIL'}: {rule.description}\",\n",
    "            affected_rows=duplicates,\n",
    "            affected_percentage=(duplicates / total_rows * 100) if total_rows > 0 else 0,\n",
    "            details={'duplicate_count': int(duplicates)}\n",
    "        )\n",
    "    \n",
    "    def _check_completeness(self, df: pd.DataFrame, rule: ValidationRule,\n",
    "                           total_rows: int) -> ValidationResult:\n",
    "        \"\"\"Check for null/missing values\"\"\"\n",
    "        null_count = df[rule.column].isnull().sum()\n",
    "        null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0\n",
    "        passed = null_pct <= (rule.threshold or 0.0)\n",
    "        \n",
    "        return ValidationResult(\n",
    "            rule_name=rule.rule_name,\n",
    "            passed=passed,\n",
    "            severity=rule.severity,\n",
    "            message=f\"{'‚úì PASS' if passed else '‚úó FAIL'}: {rule.description}\",\n",
    "            affected_rows=null_count,\n",
    "            affected_percentage=null_pct,\n",
    "            details={'null_count': int(null_count), 'null_percentage': null_pct}\n",
    "        )\n",
    "    \n",
    "    def _check_condition(self, df: pd.DataFrame, rule: ValidationRule,\n",
    "                        total_rows: int) -> ValidationResult:\n",
    "        \"\"\"Check custom condition\"\"\"\n",
    "        if rule.condition is None:\n",
    "            return ValidationResult(\n",
    "                rule_name=rule.rule_name,\n",
    "                passed=False,\n",
    "                severity='CRITICAL',\n",
    "                message=\"No condition defined for rule\"\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            valid = rule.condition(df)\n",
    "            invalid_count = (~valid).sum()\n",
    "            passed = invalid_count == 0\n",
    "            \n",
    "            return ValidationResult(\n",
    "                rule_name=rule.rule_name,\n",
    "                passed=passed,\n",
    "                severity=rule.severity,\n",
    "                message=f\"{'‚úì PASS' if passed else '‚úó FAIL'}: {rule.description}\",\n",
    "                affected_rows=invalid_count,\n",
    "                affected_percentage=(invalid_count / total_rows * 100) if total_rows > 0 else 0,\n",
    "                details={'invalid_count': int(invalid_count)}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                rule_name=rule.rule_name,\n",
    "                passed=False,\n",
    "                severity='CRITICAL',\n",
    "                message=f\"Condition evaluation failed: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    def get_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get summary of all validation results\"\"\"\n",
    "        summary_data = []\n",
    "        \n",
    "        for result in self.results:\n",
    "            summary_data.append({\n",
    "                'rule_name': result.rule_name,\n",
    "                'status': '‚úì PASS' if result.passed else '‚úó FAIL',\n",
    "                'severity': result.severity,\n",
    "                'affected_rows': result.affected_rows,\n",
    "                'affected_percentage': f\"{result.affected_percentage:.2f}%\",\n",
    "                'timestamp': result.timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "\n",
    "class DataProfiler:\n",
    "    \"\"\"Generate comprehensive data profiles\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def profile_numeric_column(series: pd.Series) -> Dict:\n",
    "        \"\"\"Profile a numeric column\"\"\"\n",
    "        return {\n",
    "            'count': int(series.count()),\n",
    "            'missing': int(series.isnull().sum()),\n",
    "            'mean': float(series.mean()),\n",
    "            'median': float(series.median()),\n",
    "            'std': float(series.std()),\n",
    "            'min': float(series.min()),\n",
    "            'max': float(series.max()),\n",
    "            'q25': float(series.quantile(0.25)),\n",
    "            'q75': float(series.quantile(0.75)),\n",
    "            'skewness': float(series.skew()),\n",
    "            'kurtosis': float(series.kurtosis())\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def profile_categorical_column(series: pd.Series) -> Dict:\n",
    "        \"\"\"Profile a categorical column\"\"\"\n",
    "        value_counts = series.value_counts()\n",
    "        \n",
    "        return {\n",
    "            'count': int(series.count()),\n",
    "            'missing': int(series.isnull().sum()),\n",
    "            'unique': int(series.nunique()),\n",
    "            'top_value': str(value_counts.index) if len(value_counts) > 0 else None,\n",
    "            'top_frequency': int(value_counts.iloc) if len(value_counts) > 0 else 0,\n",
    "            'value_distribution': value_counts.head(10).to_dict()\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def profile_date_column(series: pd.Series) -> Dict:\n",
    "        \"\"\"Profile a date column\"\"\"\n",
    "        series_dt = pd.to_datetime(series, errors='coerce')\n",
    "        \n",
    "        return {\n",
    "            'count': int(series_dt.count()),\n",
    "            'missing': int(series_dt.isnull().sum()),\n",
    "            'min_date': str(series_dt.min()),\n",
    "            'max_date': str(series_dt.max()),\n",
    "            'date_range_days': int((series_dt.max() - series_dt.min()).days) if series_dt.count() > 0 else 0\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def profile_dataframe(df: pd.DataFrame, table_name: str) -> Dict:\n",
    "        \"\"\"Generate comprehensive profile for entire DataFrame\"\"\"\n",
    "        profile = {\n",
    "            'table_name': table_name,\n",
    "            'row_count': len(df),\n",
    "            'column_count': len(df.columns),\n",
    "            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "            'duplicate_rows': int(df.duplicated().sum()),\n",
    "            'columns': {}\n",
    "        }\n",
    "        \n",
    "        for col in df.columns:\n",
    "            col_profile = {\n",
    "                'dtype': str(df[col].dtype),\n",
    "                'null_count': int(df[col].isnull().sum()),\n",
    "                'null_percentage': float(df[col].isnull().sum() / len(df) * 100)\n",
    "            }\n",
    "            \n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                col_profile['stats'] = DataProfiler.profile_numeric_column(df[col])\n",
    "            elif pd.api.types.is_datetime64_any_dtype(df[col]) or 'date' in col.lower():\n",
    "                col_profile['stats'] = DataProfiler.profile_date_column(df[col])\n",
    "            else:\n",
    "                col_profile['stats'] = DataProfiler.profile_categorical_column(df[col])\n",
    "            \n",
    "            profile['columns'][col] = col_profile\n",
    "        \n",
    "        return profile\n",
    "\n",
    "\n",
    "class AnomalyDetector:\n",
    "    \"\"\"Detect anomalies in data using statistical methods\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_outliers_iqr(series: pd.Series, multiplier: float = 1.5) -> pd.Series:\n",
    "        \"\"\"Detect outliers using IQR method\"\"\"\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - multiplier * IQR\n",
    "        upper_bound = Q3 + multiplier * IQR\n",
    "        \n",
    "        return (series < lower_bound) | (series > upper_bound)\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_outliers_zscore(series: pd.Series, threshold: float = 3.0) -> pd.Series:\n",
    "        \"\"\"Detect outliers using Z-score method\"\"\"\n",
    "        z_scores = np.abs(stats.zscore(series.dropna()))\n",
    "        return pd.Series([z > threshold for z in z_scores], index=series.dropna().index)\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_outliers_isolation_forest(df: pd.DataFrame, \n",
    "                                        columns: List[str],\n",
    "                                        contamination: float = 0.1) -> np.ndarray:\n",
    "        \"\"\"Detect multivariate outliers using Isolation Forest\"\"\"\n",
    "        X = df[columns].select_dtypes(include=[np.number]).dropna()\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            return np.array([])\n",
    "        \n",
    "        iso_forest = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_estimators=100\n",
    "        )\n",
    "        \n",
    "        predictions = iso_forest.fit_predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_table_anomalies(df: pd.DataFrame, table_name: str) -> Dict:\n",
    "        \"\"\"Comprehensive anomaly analysis for a table\"\"\"\n",
    "        results = {\n",
    "            'table_name': table_name,\n",
    "            'total_rows': len(df),\n",
    "            'columns_analyzed': [],\n",
    "            'outliers_detected': {}\n",
    "        }\n",
    "        \n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if df[col].nunique() > 10:\n",
    "                outliers_iqr = AnomalyDetector.detect_outliers_iqr(df[col])\n",
    "                outlier_count = outliers_iqr.sum()\n",
    "                \n",
    "                results['columns_analyzed'].append(col)\n",
    "                results['outliers_detected'][col] = {\n",
    "                    'method': 'IQR',\n",
    "                    'count': int(outlier_count),\n",
    "                    'percentage': float(outlier_count / len(df) * 100),\n",
    "                    'outlier_values': df[col][outliers_iqr].tolist()[:10]\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "class DataQualityReportGenerator:\n",
    "    \"\"\"Generate comprehensive data quality reports\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_html_report(validation_results: List[ValidationResult],\n",
    "                            profiles: Dict[str, Dict],\n",
    "                            anomalies: Dict[str, Dict],\n",
    "                            output_path: str = 'data_quality_report.html'):\n",
    "        \"\"\"Generate HTML data quality report\"\"\"\n",
    "        \n",
    "        passed = sum(1 for r in validation_results if r.passed)\n",
    "        failed = sum(1 for r in validation_results if not r.passed)\n",
    "        \n",
    "        validation_rows = \"\"\n",
    "        for result in validation_results:\n",
    "            status_class = 'pass' if result.passed else 'fail'\n",
    "            validation_rows += f\"\"\"\n",
    "            <tr>\n",
    "                <td><span class=\"{status_class}\">{'‚úì' if result.passed else '‚úó'}</span> {result.rule_name}</td>\n",
    "                <td class=\"{result.severity.lower()}\">{result.severity}</td>\n",
    "                <td>{result.message}</td>\n",
    "                <td>{result.affected_rows}</td>\n",
    "                <td>{result.affected_percentage:.2f}%</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Data Quality Report</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}\n",
    "        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; }}\n",
    "        h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; }}\n",
    "        .summary {{ display: flex; justify-content: space-around; margin: 20px 0; }}\n",
    "        .metric {{ text-align: center; padding: 20px; background: #ecf0f1; border-radius: 8px; }}\n",
    "        .metric-value {{ font-size: 2em; font-weight: bold; color: #2c3e50; }}\n",
    "        .pass {{ color: #27ae60; }}\n",
    "        .fail {{ color: #e74c3c; }}\n",
    "        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}\n",
    "        th {{ background: #34495e; color: white; padding: 12px; text-align: left; }}\n",
    "        td {{ padding: 10px; border-bottom: 1px solid #ecf0f1; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>üîç Data Quality Report</h1>\n",
    "        <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "        \n",
    "        <div class=\"summary\">\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value pass\">{passed}</div>\n",
    "                <div>Rules Passed</div>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value fail\">{failed}</div>\n",
    "                <div>Rules Failed</div>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{len(profiles)}</div>\n",
    "                <div>Tables Analyzed</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <h2>Validation Results</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Rule</th>\n",
    "                <th>Severity</th>\n",
    "                <th>Message</th>\n",
    "                <th>Affected Rows</th>\n",
    "                <th>Percentage</th>\n",
    "            </tr>\n",
    "            {validation_rows}\n",
    "        </table>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        return output_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Data Quality Framework - Ready for integration with ETL Pipeline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81609e47-ffd7-4cdb-8783-36fb2c842551",
   "metadata": {},
   "source": [
    "# Extract with Date Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7acd835e-cabf-4342-a59d-af9f41c2561f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'etl_pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01metl_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatabaseConnection, DataExtractor\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m ETLConfig()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'etl_pipeline'"
     ]
    }
   ],
   "source": [
    "from etl_pipeline import DatabaseConnection, DataExtractor\n",
    "from datetime import datetime\n",
    "\n",
    "config = ETLConfig()\n",
    "\n",
    "with DatabaseConnection(config, logger) as db:\n",
    "    extractor = DataExtractor(db, logger)\n",
    "    \n",
    "    # Extract sales from specific date range\n",
    "    df = extractor.extract_table(\n",
    "        'sales',\n",
    "        date_column='sale_date',\n",
    "        start_date=datetime(2025, 1, 1),\n",
    "        end_date=datetime(2026, 1, 31)\n",
    "    )\n",
    "    print(f\"Extracted {len(df)} sales records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42095e4e-586a-4638-aa57-26573f9f41cf",
   "metadata": {},
   "source": [
    "#  Custom Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d38dea0-72b6-4150-ba3b-188678e9467e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'etl_pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01metl_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataTransformer\n\u001b[1;32m      3\u001b[0m transformer \u001b[38;5;241m=\u001b[39m DataTransformer(logger)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Multiple transformation strategies\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'etl_pipeline'"
     ]
    }
   ],
   "source": [
    "from etl_pipeline import DataTransformer\n",
    "\n",
    "transformer = DataTransformer(logger)\n",
    "\n",
    "# Multiple transformation strategies\n",
    "df_clean = transformer.clean_nulls(df, strategy='drop')\n",
    "df_clean = transformer.remove_duplicates(df_clean, subset=['order_id'])\n",
    "df_clean = transformer.standardize_dates(df_clean, ['order_date', 'delivery_date'])\n",
    "df_clean = transformer.add_derived_columns(df_clean, 'orders')\n",
    "df_clean = transformer.apply_business_rules(df_clean, 'orders')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58feeff9-47c8-4496-a574-dab4ca6935cf",
   "metadata": {},
   "source": [
    "# Custom Validation Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24178558-2044-4f52-8139-9026c50a7c3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_quality_framework'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_quality_framework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataQualityRulesEngine, ValidationRule\n\u001b[1;32m      3\u001b[0m engine \u001b[38;5;241m=\u001b[39m DataQualityRulesEngine()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Add custom rule\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_quality_framework'"
     ]
    }
   ],
   "source": [
    "from data_quality_framework import DataQualityRulesEngine, ValidationRule\n",
    "\n",
    "engine = DataQualityRulesEngine()\n",
    "\n",
    "# Add custom rule\n",
    "engine.add_rule('inventory', ValidationRule(\n",
    "    rule_name='low_stock_warning',\n",
    "    rule_type='validity',\n",
    "    column='quantity_on_hand',\n",
    "    condition=lambda df: df['quantity_on_hand'] > df['reorder_level'],\n",
    "    severity='WARNING',\n",
    "    description='Stock should be above reorder point'\n",
    "))\n",
    "\n",
    "# Execute rules\n",
    "results = engine.execute_rules(df, 'inventory')\n",
    "\n",
    "# Get summary\n",
    "summary = engine.get_summary()\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240d7dbe-c51a-4612-83f4-98b4888487bf",
   "metadata": {},
   "source": [
    "# Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eaccaa-d57f-4afd-9898-967d1cf7232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_quality_framework import DataProfiler\n",
    "\n",
    "# Profile entire table\n",
    "profile = DataProfiler.profile_dataframe(df, 'products')\n",
    "\n",
    "print(f\"Table: {profile['table_name']}\")\n",
    "print(f\"Rows: {profile['row_count']:,}\")\n",
    "print(f\"Columns: {profile['column_count']}\")\n",
    "print(f\"Memory: {profile['memory_usage_mb']:.2f} MB\")\n",
    "print(f\"Duplicates: {profile['duplicate_rows']}\")\n",
    "\n",
    "# Column-level statistics\n",
    "for col, stats in profile['columns'].items():\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Nulls: {stats['null_percentage']:.2f}%\")\n",
    "    if 'stats' in stats and 'mean' in stats['stats']:\n",
    "        print(f\"  Mean: {stats['stats']['mean']:.2f}\")\n",
    "        print(f\"  Range: [{stats['stats']['min']:.2f}, {stats['stats']['max']:.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a27ddd-b35c-4c83-aea3-a6de17b70db6",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c2fa4b-1846-4a21-856a-90a7abc08b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_quality_framework import AnomalyDetector\n",
    "\n",
    "# Detect outliers using IQR method\n",
    "outliers_iqr = AnomalyDetector.detect_outliers_iqr(df['unit_cost'])\n",
    "print(f\"IQR outliers: {outliers_iqr.sum()}\")\n",
    "\n",
    "# Detect outliers using Z-score\n",
    "outliers_z = AnomalyDetector.detect_outliers_zscore(df['unit_cost'], threshold=3.0)\n",
    "print(f\"Z-score outliers: {outliers_z.sum()}\")\n",
    "\n",
    "# Multivariate anomaly detection\n",
    "predictions = AnomalyDetector.detect_outliers_isolation_forest(\n",
    "    df,\n",
    "    columns=['unit_cost', 'reorder_level', 'lead_time_days'],\n",
    "    contamination=0.1\n",
    ")\n",
    "anomalies = predictions == -1\n",
    "print(f\"Isolation Forest anomalies: {anomalies.sum()}\")\n",
    "\n",
    "# Comprehensive analysis\n",
    "report = AnomalyDetector.analyze_table_anomalies(df, 'products')\n",
    "for col, details in report['outliers_detected'].items():\n",
    "    print(f\"\\n{col}: {details['count']} outliers ({details['percentage']:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58148f-3b70-43b5-bf72-3963ff0f9b53",
   "metadata": {},
   "source": [
    "# Complete Custom Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a05d27d-eefe-4cc6-b3ed-8e6e7e571bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl_pipeline import ETLPipeline, ETLConfig, DatabaseConnection\n",
    "from etl_pipeline import DataExtractor, DataTransformer, DataQualityValidator\n",
    "from data_quality_framework import DataQualityRulesEngine, DataProfiler, AnomalyDetector\n",
    "\n",
    "config = ETLConfig()\n",
    "\n",
    "with DatabaseConnection(config, logger) as db:\n",
    "    extractor = DataExtractor(db, logger)\n",
    "    transformer = DataTransformer(logger)\n",
    "    validator = DataQualityValidator(db, config, logger)\n",
    "    \n",
    "    # Extract\n",
    "    df = extractor.extract_table('products')\n",
    "    print(f\"Extracted: {len(df)} rows\")\n",
    "    \n",
    "    # Transform\n",
    "    df = transformer.clean_nulls(df, strategy='drop')\n",
    "    df = transformer.remove_duplicates(df)\n",
    "    df = transformer.apply_business_rules(df, 'products')\n",
    "    print(f\"Cleaned: {len(df)} rows\")\n",
    "    \n",
    "    # Validate with custom rules\n",
    "    rules_engine = DataQualityRulesEngine()\n",
    "    rules_engine.define_standard_rules()\n",
    "    validation_results = rules_engine.execute_rules(df, 'products')\n",
    "    \n",
    "    passed = sum(1 for r in validation_results if r.passed)\n",
    "    total = len(validation_results)\n",
    "    print(f\"Validation: {passed}/{total} rules passed\")\n",
    "    \n",
    "    # Profile\n",
    "    profile = DataProfiler.profile_dataframe(df, 'products')\n",
    "    print(f\"Profile: {profile['column_count']} columns analyzed\")\n",
    "    \n",
    "    # Detect anomalies\n",
    "    anomalies = AnomalyDetector.analyze_table_anomalies(df, 'products')\n",
    "    outlier_count = sum([v['count'] for v in anomalies['outliers_detected'].values()])\n",
    "    print(f\"Anomalies: {outlier_count} outliers detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2778e-cc37-4dc9-a5a5-48e313aa1aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e4c164-884d-46cc-915d-d31bc8683f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3d1c1-299d-4395-994c-731daf494135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f1546-31d4-4a42-852e-586ab50edd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
