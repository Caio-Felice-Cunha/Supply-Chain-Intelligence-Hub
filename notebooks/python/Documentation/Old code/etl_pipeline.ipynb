{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d914887-b11e-4985-876e-a14fbc070548",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ETL PIPELINE INITIALIZED\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Connected to database: supply_chain_db\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - \n",
      "======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Processing table: suppliers\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Extracting data from table: suppliers\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Extracted 30 rows from suppliers\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ No duplicates found\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Standardized date column: created_date\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Adding derived columns for suppliers\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Applying business rules for suppliers\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - === Validating data quality for: suppliers ===\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Data quality validation PASSED for suppliers\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Successfully processed suppliers\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - \n",
      "======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Processing table: products\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Extracting data from table: products\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Extracted 46 rows from products\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ No duplicates found\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Adding derived columns for products\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Applying business rules for products\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - === Validating data quality for: products ===\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Data quality validation PASSED for products\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Successfully processed products\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - \n",
      "======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Processing table: warehouses\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Extracting data from table: warehouses\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Extracted 18 rows from warehouses\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ No duplicates found\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Adding derived columns for warehouses\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Applying business rules for warehouses\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - === Validating data quality for: warehouses ===\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Data quality validation PASSED for warehouses\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Successfully processed warehouses\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - \n",
      "======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Processing table: inventory\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Extracting data from table: inventory\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Extracted 263 rows from inventory\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ No duplicates found\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Standardized date column: snapshot_date\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Adding derived columns for inventory\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Added 'quantity_available' column\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Applying business rules for inventory\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - === Validating data quality for: inventory ===\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Data quality validation PASSED for inventory\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Successfully processed inventory\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - \n",
      "======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Processing table: orders\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Extracting data from table: orders\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Extracted 67 rows from orders\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ No duplicates found\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Standardized date column: order_date\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Standardized date column: expected_delivery_date\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Standardized date column: actual_delivery_date\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Adding derived columns for orders\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Added delivery metrics columns\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Applying business rules for orders\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - === Validating data quality for: orders ===\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - WARNING - ⚠ High null count in 'actual_delivery_date': 31 (46.3%)\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - WARNING - ⚠ High null count in 'delivery_delay_days': 31 (46.3%)\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - ERROR - ✗ Data quality validation FAILED for orders\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - WARNING - ⚠ Quality validation failed for orders, skipping load\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - \n",
      "======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Processing table: sales\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Extracting data from table: sales\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Extracted 166 rows from sales\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ No duplicates found\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Standardized date column: sale_date\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Adding derived columns for sales\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Added 'unit_price' column\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Applying business rules for sales\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - === Validating data quality for: sales ===\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Data quality validation PASSED for sales\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Successfully processed sales\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - \n",
      "======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Processing table: price_history\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Extracting data from table: price_history\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Extracted 110 rows from price_history\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ No duplicates found\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Standardized date column: effective_date\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Adding derived columns for price_history\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - Applying business rules for price_history\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - === Validating data quality for: price_history ===\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Data quality validation PASSED for price_history\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Successfully processed price_history\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - \n",
      "======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ETL PIPELINE COMPLETED\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ======================================================================\n",
      "2026-02-01 17:53:06 - ETL_Pipeline - INFO - ✓ Database connection closed\n",
      "\n",
      "✗ Pipeline completed with errors\n",
      "\n",
      "Processed: 6 tables\n",
      "Failed: 1 tables\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Supply Chain Intelligence Hub - ETL Pipeline\n",
    "============================================\n",
    "Production-grade ETL pipeline with comprehensive error handling,\n",
    "data quality validation, and logging.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "from sqlalchemy.exc import SQLAlchemyError, IntegrityError\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#Connecting with SQLAlchemy\n",
    "# SQLAlchemy connection string (inside Docker network)\n",
    "DB_USER = \"analytics_user\"\n",
    "DB_PASS = \"analyticspass123\"\n",
    "DB_HOST = \"mysql\"          # service name from docker-compose\n",
    "DB_PORT = \"3306\"\n",
    "DB_NAME = \"supply_chain_db\"\n",
    "\n",
    "connection_string = f\"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Quick test: list tables\n",
    "with engine.connect() as conn:\n",
    "    tables = pd.read_sql(\"SHOW TABLES;\", conn)\n",
    "tables\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class ETLConfig:\n",
    "    \"\"\"Configuration for ETL pipeline\"\"\"\n",
    "    db_host: str = \"mysql\"\n",
    "    db_port: int = 3306\n",
    "    db_user: str = \"analytics_user\"\n",
    "    db_password: str = \"analyticspass123\"\n",
    "    db_name: str = \"supply_chain_db\"\n",
    "    batch_size: int = 1000\n",
    "    max_retries: int = 3\n",
    "    retry_delay: int = 5\n",
    "    null_threshold: float = 0.05\n",
    "    duplicate_threshold: float = 0.01\n",
    "    log_level: str = \"INFO\"\n",
    "    log_file: str = \"etl_pipeline.log\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataQualityReport:\n",
    "    \"\"\"Data quality validation results\"\"\"\n",
    "    table_name: str\n",
    "    total_rows: int\n",
    "    null_count: Dict[str, int] = field(default_factory=dict)\n",
    "    duplicate_count: int = 0\n",
    "    missing_foreign_keys: Dict[str, int] = field(default_factory=dict)\n",
    "    validation_passed: bool = True\n",
    "    issues: List[str] = field(default_factory=list)\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "    \n",
    "    def add_issue(self, issue: str):\n",
    "        self.issues.append(issue)\n",
    "        self.validation_passed = False\n",
    "\n",
    "\n",
    "def setup_logging(config: ETLConfig) -> logging.Logger:\n",
    "    \"\"\"Configure logging with file and console handlers\"\"\"\n",
    "    logger = logging.getLogger('ETL_Pipeline')\n",
    "    logger.setLevel(getattr(logging, config.log_level))\n",
    "    logger.handlers = []\n",
    "    \n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_format = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    console_handler.setFormatter(console_format)\n",
    "    \n",
    "    file_handler = logging.FileHandler(config.log_file)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_format = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n",
    "    )\n",
    "    file_handler.setFormatter(file_format)\n",
    "    \n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "\n",
    "class DatabaseConnection:\n",
    "    \"\"\"Context manager for database connections with error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ETLConfig, logger: logging.Logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.engine = None\n",
    "        self.connection = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        try:\n",
    "            connection_string = (\n",
    "                f\"mysql+pymysql://{self.config.db_user}:{self.config.db_password}\"\n",
    "                f\"@{self.config.db_host}:{self.config.db_port}/{self.config.db_name}\"\n",
    "            )\n",
    "            self.engine = create_engine(\n",
    "                connection_string,\n",
    "                pool_pre_ping=True,\n",
    "                pool_recycle=3600,\n",
    "                echo=False\n",
    "            )\n",
    "            self.connection = self.engine.connect()\n",
    "            self.logger.info(f\"✓ Connected to database: {self.config.db_name}\")\n",
    "            return self\n",
    "        except SQLAlchemyError as e:\n",
    "            self.logger.error(f\"✗ Database connection failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            self.logger.info(\"✓ Database connection closed\")\n",
    "        if exc_type:\n",
    "            self.logger.error(f\"✗ Error during database operation: {exc_val}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "class DataExtractor:\n",
    "    \"\"\"Extract data from various sources\"\"\"\n",
    "    \n",
    "    def __init__(self, connection: DatabaseConnection, logger: logging.Logger):\n",
    "        self.connection = connection\n",
    "        self.logger = logger\n",
    "    \n",
    "    def extract_table(self, table_name: str, \n",
    "                     date_column: Optional[str] = None,\n",
    "                     start_date: Optional[datetime] = None,\n",
    "                     end_date: Optional[datetime] = None) -> pd.DataFrame:\n",
    "        try:\n",
    "            self.logger.info(f\"Extracting data from table: {table_name}\")\n",
    "            \n",
    "            query = f\"SELECT * FROM {table_name}\"\n",
    "            \n",
    "            if date_column and start_date and end_date:\n",
    "                query += f\" WHERE {date_column} BETWEEN :start_date AND :end_date\"\n",
    "                params = {'start_date': start_date, 'end_date': end_date}\n",
    "                df = pd.read_sql_query(text(query), self.connection.connection, params=params)\n",
    "            else:\n",
    "                df = pd.read_sql_query(query, self.connection.connection)\n",
    "            \n",
    "            self.logger.info(f\"✓ Extracted {len(df):,} rows from {table_name}\")\n",
    "            return df\n",
    "        except SQLAlchemyError as e:\n",
    "            self.logger.error(f\"✗ Failed to extract from {table_name}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def extract_with_joins(self, query: str, params: Optional[Dict] = None) -> pd.DataFrame:\n",
    "        try:\n",
    "            self.logger.info(\"Executing custom extraction query\")\n",
    "            if params:\n",
    "                df = pd.read_sql_query(text(query), self.connection.connection, params=params)\n",
    "            else:\n",
    "                df = pd.read_sql_query(query, self.connection.connection)\n",
    "            self.logger.info(f\"✓ Extracted {len(df):,} rows from custom query\")\n",
    "            return df\n",
    "        except SQLAlchemyError as e:\n",
    "            self.logger.error(f\"✗ Custom query failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class DataTransformer:\n",
    "    \"\"\"Transform and clean extracted data\"\"\"\n",
    "    \n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        self.logger = logger\n",
    "    \n",
    "    def clean_nulls(self, df: pd.DataFrame, strategy: str = 'drop') -> pd.DataFrame:\n",
    "        null_count = df.isnull().sum().sum()\n",
    "        self.logger.info(f\"Handling {null_count} null values using strategy: {strategy}\")\n",
    "        \n",
    "        if strategy == 'drop':\n",
    "            return df.dropna()\n",
    "        elif strategy == 'fill_mean':\n",
    "            return df.fillna(df.mean(numeric_only=True))\n",
    "        elif strategy == 'fill_median':\n",
    "            return df.fillna(df.median(numeric_only=True))\n",
    "        elif strategy == 'fill_forward':\n",
    "            return df.fillna(method='ffill')\n",
    "        else:\n",
    "            self.logger.warning(f\"Unknown strategy '{strategy}', returning original DataFrame\")\n",
    "            return df\n",
    "    \n",
    "    def remove_duplicates(self, df: pd.DataFrame, \n",
    "                         subset: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        initial_rows = len(df)\n",
    "        df_clean = df.drop_duplicates(subset=subset, keep='first')\n",
    "        removed = initial_rows - len(df_clean)\n",
    "        \n",
    "        if removed > 0:\n",
    "            self.logger.warning(f\"Removed {removed} duplicate rows\")\n",
    "        else:\n",
    "            self.logger.info(\"✓ No duplicates found\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def standardize_dates(self, df: pd.DataFrame, \n",
    "                         date_columns: List[str],\n",
    "                         date_format: str = '%Y-%m-%d') -> pd.DataFrame:\n",
    "        for col in date_columns:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                    self.logger.info(f\"✓ Standardized date column: {col}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"✗ Failed to standardize {col}: {str(e)}\")\n",
    "        return df\n",
    "    \n",
    "    def add_derived_columns(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "        self.logger.info(f\"Adding derived columns for {table_name}\")\n",
    "        \n",
    "        if table_name == 'inventory':\n",
    "            if 'quantity_on_hand' in df.columns and 'quantity_reserved' in df.columns:\n",
    "                df['quantity_available'] = df['quantity_on_hand'] - df['quantity_reserved']\n",
    "                self.logger.info(\"✓ Added 'quantity_available' column\")\n",
    "        \n",
    "        elif table_name == 'orders':\n",
    "            if 'expected_delivery_date' in df.columns and 'actual_delivery_date' in df.columns:\n",
    "                df['delivery_delay_days'] = (\n",
    "                    pd.to_datetime(df['actual_delivery_date']) - \n",
    "                    pd.to_datetime(df['expected_delivery_date'])\n",
    "                ).dt.days\n",
    "                df['is_late'] = df['delivery_delay_days'] > 0\n",
    "                self.logger.info(\"✓ Added delivery metrics columns\")\n",
    "        \n",
    "        elif table_name == 'sales':\n",
    "            if 'revenue' in df.columns and 'quantity_sold' in df.columns:\n",
    "                df['unit_price'] = df['revenue'] / df['quantity_sold']\n",
    "                self.logger.info(\"✓ Added 'unit_price' column\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def apply_business_rules(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "        self.logger.info(f\"Applying business rules for {table_name}\")\n",
    "        initial_rows = len(df)\n",
    "        \n",
    "        if table_name == 'products':\n",
    "            df = df[df['unit_cost'] > 0]\n",
    "            df = df[df['reorder_level'] >= 0]\n",
    "        elif table_name == 'inventory':\n",
    "            df = df[df['quantity_on_hand'] >= 0]\n",
    "            df = df[df['quantity_reserved'] >= 0]\n",
    "        elif table_name == 'orders':\n",
    "            df = df[df['order_quantity'] > 0]\n",
    "            df = df[df['order_cost'] >= 0]\n",
    "        elif table_name == 'sales':\n",
    "            df = df[df['quantity_sold'] > 0]\n",
    "            df = df[df['revenue'] >= 0]\n",
    "        \n",
    "        removed = initial_rows - len(df)\n",
    "        if removed > 0:\n",
    "            self.logger.warning(f\"Removed {removed} rows violating business rules\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "class DataQualityValidator:\n",
    "    \"\"\"Comprehensive data quality checks\"\"\"\n",
    "    \n",
    "    def __init__(self, connection: DatabaseConnection, \n",
    "                 config: ETLConfig, logger: logging.Logger):\n",
    "        self.connection = connection\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "    \n",
    "    def validate_table(self, df: pd.DataFrame, \n",
    "                      table_name: str,\n",
    "                      required_columns: Optional[List[str]] = None) -> DataQualityReport:\n",
    "        self.logger.info(f\"=== Validating data quality for: {table_name} ===\")\n",
    "        report = DataQualityReport(table_name=table_name, total_rows=len(df))\n",
    "        \n",
    "        if required_columns:\n",
    "            missing_cols = set(required_columns) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                report.add_issue(f\"Missing required columns: {missing_cols}\")\n",
    "                self.logger.error(f\"✗ Missing columns: {missing_cols}\")\n",
    "        \n",
    "        null_counts = df.isnull().sum()\n",
    "        for col, count in null_counts.items():\n",
    "            if count > 0:\n",
    "                null_pct = count / len(df)\n",
    "                report.null_count[col] = count\n",
    "                \n",
    "                if null_pct > self.config.null_threshold:\n",
    "                    report.add_issue(\n",
    "                        f\"Column '{col}' has {null_pct:.1%} nulls (threshold: {self.config.null_threshold:.1%})\"\n",
    "                    )\n",
    "                    self.logger.warning(f\"⚠ High null count in '{col}': {count} ({null_pct:.1%})\")\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            duplicate_count = df.duplicated().sum()\n",
    "            report.duplicate_count = duplicate_count\n",
    "            \n",
    "            if duplicate_count > 0:\n",
    "                dup_pct = duplicate_count / len(df)\n",
    "                if dup_pct > self.config.duplicate_threshold:\n",
    "                    report.add_issue(f\"Found {duplicate_count} duplicates ({dup_pct:.1%})\")\n",
    "                    self.logger.warning(f\"⚠ Duplicates found: {duplicate_count}\")\n",
    "        \n",
    "        fk_issues = self._validate_foreign_keys(df, table_name)\n",
    "        if fk_issues:\n",
    "            report.missing_foreign_keys = fk_issues\n",
    "            for fk, count in fk_issues.items():\n",
    "                report.add_issue(f\"Missing foreign key references in '{fk}': {count} rows\")\n",
    "                self.logger.error(f\"✗ Foreign key issue in '{fk}': {count} orphaned rows\")\n",
    "        \n",
    "        if report.validation_passed:\n",
    "            self.logger.info(f\"✓ Data quality validation PASSED for {table_name}\")\n",
    "        else:\n",
    "            self.logger.error(f\"✗ Data quality validation FAILED for {table_name}\")\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _validate_foreign_keys(self, df: pd.DataFrame, table_name: str) -> Dict[str, int]:\n",
    "        issues = {}\n",
    "        fk_mapping = {\n",
    "            'products': {'supplier_id': 'suppliers'},\n",
    "            'inventory': {'product_id': 'products', 'warehouse_id': 'warehouses'},\n",
    "            'orders': {'supplier_id': 'suppliers'},\n",
    "            'sales': {'product_id': 'products', 'warehouse_id': 'warehouses'},\n",
    "            'price_history': {'product_id': 'products', 'supplier_id': 'suppliers'}\n",
    "        }\n",
    "        \n",
    "        if table_name not in fk_mapping:\n",
    "            return issues\n",
    "        \n",
    "        for fk_col, parent_table in fk_mapping[table_name].items():\n",
    "            if fk_col not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                parent_ids = pd.read_sql_query(\n",
    "                    f\"SELECT {fk_col.replace('_id', '')}_id FROM {parent_table}\",\n",
    "                    self.connection.connection\n",
    "                )\n",
    "                valid_ids = set(parent_ids.iloc[:, 0])\n",
    "                orphaned = ~df[fk_col].isin(valid_ids)\n",
    "                orphaned_count = orphaned.sum()\n",
    "                \n",
    "                if orphaned_count > 0:\n",
    "                    issues[fk_col] = orphaned_count\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Could not validate FK {fk_col}: {str(e)}\")\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def generate_quality_summary(self, reports: List[DataQualityReport]) -> pd.DataFrame:\n",
    "        summary_data = []\n",
    "        for report in reports:\n",
    "            summary_data.append({\n",
    "                'table_name': report.table_name,\n",
    "                'total_rows': report.total_rows,\n",
    "                'null_columns': len(report.null_count),\n",
    "                'total_nulls': sum(report.null_count.values()),\n",
    "                'duplicates': report.duplicate_count,\n",
    "                'fk_issues': len(report.missing_foreign_keys),\n",
    "                'validation_passed': report.validation_passed,\n",
    "                'issue_count': len(report.issues),\n",
    "                'timestamp': report.timestamp\n",
    "            })\n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Load transformed data into database\"\"\"\n",
    "    \n",
    "    def __init__(self, connection: DatabaseConnection, \n",
    "                 config: ETLConfig, logger: logging.Logger):\n",
    "        self.connection = connection\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "    \n",
    "    def load_data(self, df: pd.DataFrame, table_name: str,\n",
    "                  if_exists: str = 'append',\n",
    "                  create_backup: bool = True) -> Tuple[int, int]:\n",
    "        self.logger.info(f\"Loading {len(df)} rows into table: {table_name}\")\n",
    "        \n",
    "        rows_loaded = 0\n",
    "        rows_failed = 0\n",
    "        \n",
    "        try:\n",
    "            if create_backup and if_exists == 'replace':\n",
    "                self._create_backup(table_name)\n",
    "            \n",
    "            for i in range(0, len(df), self.config.batch_size):\n",
    "                batch = df.iloc[i:i + self.config.batch_size]\n",
    "                \n",
    "                try:\n",
    "                    batch.to_sql(\n",
    "                        name=table_name,\n",
    "                        con=self.connection.engine,\n",
    "                        if_exists=if_exists if i == 0 else 'append',\n",
    "                        index=False,\n",
    "                        method='multi'\n",
    "                    )\n",
    "                    rows_loaded += len(batch)\n",
    "                except IntegrityError as e:\n",
    "                    self.logger.error(f\"Integrity error in batch: {str(e)}\")\n",
    "                    rows_failed += len(batch)\n",
    "                except SQLAlchemyError as e:\n",
    "                    self.logger.error(f\"Database error in batch: {str(e)}\")\n",
    "                    rows_failed += len(batch)\n",
    "            \n",
    "            self.logger.info(f\"✓ Load completed: {rows_loaded} rows loaded, {rows_failed} rows failed\")\n",
    "            return rows_loaded, rows_failed\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"✗ Load failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _create_backup(self, table_name: str):\n",
    "        backup_name = f\"{table_name}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        try:\n",
    "            with self.connection.connection.begin():\n",
    "                self.connection.connection.execute(\n",
    "                    text(f\"CREATE TABLE {backup_name} AS SELECT * FROM {table_name}\")\n",
    "                )\n",
    "            self.logger.info(f\"✓ Created backup table: {backup_name}\")\n",
    "        except SQLAlchemyError as e:\n",
    "            self.logger.warning(f\"Could not create backup: {str(e)}\")\n",
    "\n",
    "\n",
    "class ETLPipeline:\n",
    "    \"\"\"Main ETL pipeline orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Optional[ETLConfig] = None):\n",
    "        self.config = config or ETLConfig()\n",
    "        self.logger = setup_logging(self.config)\n",
    "        self.quality_reports = []\n",
    "        \n",
    "        self.logger.info(\"=\" * 70)\n",
    "        self.logger.info(\"ETL PIPELINE INITIALIZED\")\n",
    "        self.logger.info(\"=\" * 70)\n",
    "    \n",
    "    def run_full_pipeline(self, tables: List[str]) -> Dict[str, any]:\n",
    "        results = {\n",
    "            'success': False,\n",
    "            'tables_processed': [],\n",
    "            'tables_failed': [],\n",
    "            'quality_reports': [],\n",
    "            'summary': {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with DatabaseConnection(self.config, self.logger) as db:\n",
    "                extractor = DataExtractor(db, self.logger)\n",
    "                transformer = DataTransformer(self.logger)\n",
    "                validator = DataQualityValidator(db, self.config, self.logger)\n",
    "                loader = DataLoader(db, self.config, self.logger)\n",
    "                \n",
    "                for table in tables:\n",
    "                    try:\n",
    "                        self.logger.info(f\"\\n{'='*70}\")\n",
    "                        self.logger.info(f\"Processing table: {table}\")\n",
    "                        self.logger.info(f\"{'='*70}\")\n",
    "                        \n",
    "                        df = extractor.extract_table(table)\n",
    "                        # df = transformer.clean_nulls(df, strategy='drop')\n",
    "                        df = transformer.remove_duplicates(df)\n",
    "                        df = transformer.standardize_dates(\n",
    "                            df, \n",
    "                            date_columns=[col for col in df.columns if 'date' in col.lower()]\n",
    "                        )\n",
    "                        df = transformer.add_derived_columns(df, table)\n",
    "                        df = transformer.apply_business_rules(df, table)\n",
    "                        \n",
    "                        report = validator.validate_table(df, table)\n",
    "                        self.quality_reports.append(report)\n",
    "                        \n",
    "                        if not report.validation_passed:\n",
    "                            self.logger.warning(f\"⚠ Quality validation failed for {table}, skipping load\")\n",
    "                            results['tables_failed'].append(table)\n",
    "                            continue\n",
    "                        \n",
    "                        results['tables_processed'].append(table)\n",
    "                        self.logger.info(f\"✓ Successfully processed {table}\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"✗ Failed to process {table}: {str(e)}\")\n",
    "                        results['tables_failed'].append(table)\n",
    "                \n",
    "                results['quality_reports'] = self.quality_reports\n",
    "                results['summary'] = validator.generate_quality_summary(self.quality_reports)\n",
    "                results['success'] = len(results['tables_failed']) == 0\n",
    "                \n",
    "                self.logger.info(\"\\n\" + \"=\"*70)\n",
    "                self.logger.info(\"ETL PIPELINE COMPLETED\")\n",
    "                self.logger.info(\"=\"*70)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"✗ Pipeline failed: {str(e)}\")\n",
    "            results['success'] = False\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = ETLConfig()\n",
    "    pipeline = ETLPipeline(config)\n",
    "    tables_to_process = ['suppliers', 'products', 'warehouses', 'inventory', 'orders', 'sales', 'price_history']\n",
    "    results = pipeline.run_full_pipeline(tables_to_process)\n",
    "    \n",
    "    if results['success']:\n",
    "        print(\"\\n✓ Pipeline completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n✗ Pipeline completed with errors\")\n",
    "    \n",
    "    print(f\"\\nProcessed: {len(results['tables_processed'])} tables\")\n",
    "    print(f\"Failed: {len(results['tables_failed'])} tables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8df8f-b0af-4e48-baf3-97ef68502d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
